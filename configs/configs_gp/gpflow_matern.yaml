
 
OUTPUT: 
  # OUTPUT_DIR: "/shared/tale2/Shared/schobs/landmark_unet/lannUnet_exps/GP/gpflow_196" #Change this to where you want to save model checkpoints/results

  OUTPUT_DIR: '/home/schobs/Documents/PhD/local_LaNNU-Net/outputs/gp/gpflow_196' #change this to where you have the data

  USE_COMETML_LOGGING: True
  COMET_API_KEY: "B5Nk91E6iCmWvBznXXq3Ijhhp" #get a comet.ml account
  COMET_WORKSPACE: "schobs"
  VERBOSE: True #whether to print results every epoch

SOLVER:
  DEEP_SUPERVISION: False  # Leave this.
  NUM_RES_SUPERVISIONS: 1 # Leave this.
  LOSS_FUNCTION: "mse"
  DATA_LOADER_BATCH_SIZE_TRAIN: -1 # indicates entire dataset for batch (needed for ExactGP)
  MAX_EPOCHS: 15000 
  MINI_BATCH_SIZE: 1 # Leave this.
  # REGRESS_SIGMA: True
  DATA_LOADER_BATCH_SIZE_EVAL: -1 
  BASE_LR: 0.01
  AUTO_MIXED_PRECISION: False
  EARLY_STOPPING_PATIENCE: 5000 # epochs for validation coord error not improving before stopping training




DATASET:
  ROOT: '/home/schobs/Documents/PhD/data/ISBI2015_landmarks/' #change this to where you have the data
  SRC_TARGETS: '/home/schobs/Documents/PhD/data/ISBI2015_landmarks/lann_folds/w_valid' #change this
  NAME:  "ISBI 2015 Junior"
  IMAGE_MODALITY: 'Cephalometric'
  LANDMARKS : [0] #Choose landmark (between 0-17)
  TRAINSET_SIZE: -1 # -1 for full trainset size or int <= len(training_set). USeful for debugging if you don't wanna load entire dataset.
  TO_PYTORCH_TENSOR: False  # True if using pytorch, False if using tensorflow

 
TRAINER:
  PERFORM_VALIDATION: True
  SAVE_LATEST_ONLY: False
  CACHE_DATA: False # Kind of irrelevant for ExactGP
  INFERENCE_ONLY: False # Set True and add a MODEL.CHECKPOINT to load a model and run inference only.
  SAVE_EVERY: 500 # Saves checkpoint every X epochs
  VALIDATE_EVERY: 1 #validation every X epochs

SAMPLER:
  DATA_AUG: "Flatten" #the data aug scheme, currently flattens, but can change this to other schemes.
  INPUT_SIZE : [512,512] #resizes data to this size, then gets patches of SAMPLER.PATCH.SAMPLE_PATCH_SIZE
  SAMPLE_MODE: 'patch_centred' # ['patch_bias', 'full', "patch_centred"] (patch_centred gets centre from stage 1 preds)
  EVALUATION_SAMPLE_MODE: "patch_centred"  # ['patch_bias', 'full', "patch_centred"] (evaluate from stage 1 preds)

  NUM_WORKERS: 0
  DEBUG: False # Set True for nice visualizations and logging. recomended for understanding.
  PATCH:
    SAMPLER_BIAS: 1.0 #irrelevent for patch_centred
    RESOLUTION_TO_SAMPLE_FROM:  "input_size" #means it samples from 512,512 not the original resolution
    SAMPLE_PATCH_SIZE: [128,128] #patch size
    # CENTRED_PATCH_COORDINATE_PATH: "/shared/tale2/Shared/schobs/landmark_unet/lannUnet_exps/ISBI/param_search/ISBI_512F_512Res_5GS_4MFR_AugAS_DS5/individual_results_allfolds.xlsx"
    CENTRED_PATCH_COORDINATE_PATH: "/shared/tale2/Shared/schobs/landmark_unet/lannUnet_exps/GP/unet_s1_preds_all/individual_results_fold0.xlsx"
    CENTRED_PATCH_COORDINATE_PATH_SHEET: "model_best_valid_coord_error_f0"
    CENTRED_PATCH_JITTER: 1.0 #jitter for patch_centred. is a proportion of the patch size, which will generate random x,y jitter capped at that proportion .

MODEL:
  GAUSS_SIGMA: 2 #irrelevant
  ARCHITECTURE: "GPFlow"  # ["U-Net" , "PHD-Net"]
  CHECKPOINT: "/home/schobs/Documents/PhD/local_LaNNU-Net/outputs/gp/gpflow_196/model_best_valid_loss_fold0.model" #Put your checkpoint for Inference here.
  GP:
    KERN: "matern"
    
INFERENCE:
  EVALUATION_MODE: "use_input_size"  # ["scale_heatmap_first", "scale_pred_coords", "use_input_size"]
  DEBUG: True #Set True for plot of predictions with covariances plotted.
   
